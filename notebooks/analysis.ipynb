{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Science Collaboration Project - Analysis Notebook\n",
        "\n",
        "This notebook demonstrates the complete data science workflow using our collaborative project structure.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Environment Setup](#environment-setup)\n",
        "2. [Data Loading and Exploration](#data-loading-and-exploration)\n",
        "3. [Data Preprocessing](#data-preprocessing)\n",
        "4. [Exploratory Data Analysis](#exploratory-data-analysis)\n",
        "5. [Model Training and Evaluation](#model-training-and-evaluation)\n",
        "6. [Results and Conclusions](#results-and-conclusions)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "First, let's import all necessary libraries and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src directory to path for importing our modules\n",
        "sys.path.append('../src')\n",
        "\n",
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Our custom modules\n",
        "from data_preprocessing import preprocess_pipeline, load_raw_data, clean_missing_values\n",
        "from model_training import ModelTrainer\n",
        "from utils import setup_plotting_style, check_data_quality, create_data_profile\n",
        "\n",
        "# Set up plotting style\n",
        "setup_plotting_style()\n",
        "\n",
        "print(\"Environment setup complete!\")\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Exploration\n",
        "\n",
        "Since we don't have real data yet, let's create a sample dataset to demonstrate our workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample dataset for demonstration\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic customer data\n",
        "n_samples = 1000\n",
        "\n",
        "data = {\n",
        "    'customer_id': range(1, n_samples + 1),\n",
        "    'age': np.random.normal(35, 12, n_samples).astype(int),\n",
        "    'income': np.random.normal(50000, 15000, n_samples),\n",
        "    'spending_score': np.random.normal(50, 25, n_samples),\n",
        "    'years_as_customer': np.random.exponential(3, n_samples),\n",
        "    'num_purchases': np.random.poisson(12, n_samples),\n",
        "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples),\n",
        "    'gender': np.random.choice(['M', 'F'], n_samples),\n",
        "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples, p=[0.3, 0.4, 0.2, 0.1])\n",
        "}\n",
        "\n",
        "# Create target variable (high-value customer)\n",
        "# Based on income, spending_score, and num_purchases\n",
        "high_value = ((data['income'] > 55000) & \n",
        "              (data['spending_score'] > 60) & \n",
        "              (data['num_purchases'] > 15)).astype(int)\n",
        "\n",
        "data['high_value_customer'] = high_value\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Introduce some missing values for demonstration\n",
        "missing_indices = np.random.choice(df.index, size=50, replace=False)\n",
        "df.loc[missing_indices, 'income'] = np.nan\n",
        "\n",
        "missing_indices = np.random.choice(df.index, size=30, replace=False)\n",
        "df.loc[missing_indices, 'spending_score'] = np.nan\n",
        "\n",
        "print(\"Sample dataset created!\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic dataset information\n",
        "print(\"Dataset Info:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(f\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "print(f\"\\nBasic statistics:\")\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Quality Assessment\n",
        "\n",
        "Let's use our custom utility functions to assess data quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform data quality check using our custom function\n",
        "quality_report = check_data_quality(df)\n",
        "\n",
        "print(\"Data Quality Report Summary:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Total missing values: {sum(quality_report['missing_values']['total_missing'].values())}\")\n",
        "print(f\"Duplicate rows: {quality_report['duplicates']['duplicate_rows']}\")\n",
        "print(f\"Potential issues found: {len(quality_report['potential_issues'])}\")\n",
        "\n",
        "if quality_report['potential_issues']:\n",
        "    print(\"\\nPotential Issues:\")\n",
        "    for issue in quality_report['potential_issues']:\n",
        "        print(f\"- {issue}\")\n",
        "else:\n",
        "    print(\"No potential issues detected!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data profile visualization\n",
        "create_data_profile(df, \"Customer Dataset Profile\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Exploratory Data Analysis\n",
        "\n",
        "Let's explore the relationships in our data and understand the patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of target variable\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "df['high_value_customer'].value_counts().plot(kind='bar', color=['lightcoral', 'skyblue'])\n",
        "plt.title('Distribution of High-Value Customers')\n",
        "plt.xlabel('High Value Customer')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks([0, 1], ['No', 'Yes'], rotation=0)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "target_pct = df['high_value_customer'].value_counts(normalize=True) * 100\n",
        "plt.pie(target_pct.values, labels=['No', 'Yes'], autopct='%1.1f%%', colors=['lightcoral', 'skyblue'])\n",
        "plt.title('Percentage of High-Value Customers')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"High-value customers: {df['high_value_customer'].sum()} ({df['high_value_customer'].mean():.1%})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation analysis\n",
        "from utils import correlation_analysis\n",
        "\n",
        "print(\"Performing correlation analysis...\")\n",
        "high_corr_pairs = correlation_analysis(df, threshold=0.3, plot=True)\n",
        "\n",
        "if len(high_corr_pairs) > 0:\n",
        "    print(\"\\nHighly correlated feature pairs:\")\n",
        "    print(high_corr_pairs.round(3))\n",
        "else:\n",
        "    print(\"No highly correlated features found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature distributions by target variable\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Feature Distributions by High-Value Customer Status', fontsize=16)\n",
        "\n",
        "# Age distribution\n",
        "axes[0, 0].hist(df[df['high_value_customer'] == 0]['age'], alpha=0.7, label='Not High-Value', bins=20)\n",
        "axes[0, 0].hist(df[df['high_value_customer'] == 1]['age'], alpha=0.7, label='High-Value', bins=20)\n",
        "axes[0, 0].set_title('Age Distribution')\n",
        "axes[0, 0].set_xlabel('Age')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# Income distribution\n",
        "axes[0, 1].hist(df[df['high_value_customer'] == 0]['income'].dropna(), alpha=0.7, label='Not High-Value', bins=20)\n",
        "axes[0, 1].hist(df[df['high_value_customer'] == 1]['income'].dropna(), alpha=0.7, label='High-Value', bins=20)\n",
        "axes[0, 1].set_title('Income Distribution')\n",
        "axes[0, 1].set_xlabel('Income')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# Spending Score distribution\n",
        "axes[1, 0].hist(df[df['high_value_customer'] == 0]['spending_score'].dropna(), alpha=0.7, label='Not High-Value', bins=20)\n",
        "axes[1, 0].hist(df[df['high_value_customer'] == 1]['spending_score'].dropna(), alpha=0.7, label='High-Value', bins=20)\n",
        "axes[1, 0].set_title('Spending Score Distribution')\n",
        "axes[1, 0].set_xlabel('Spending Score')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# Number of purchases distribution\n",
        "axes[1, 1].hist(df[df['high_value_customer'] == 0]['num_purchases'], alpha=0.7, label='Not High-Value', bins=20)\n",
        "axes[1, 1].hist(df[df['high_value_customer'] == 1]['num_purchases'], alpha=0.7, label='High-Value', bins=20)\n",
        "axes[1, 1].set_title('Number of Purchases Distribution')\n",
        "axes[1, 1].set_xlabel('Number of Purchases')\n",
        "axes[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training and Evaluation\n",
        "\n",
        "Now let's use our custom model training module to build and evaluate machine learning models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for modeling\n",
        "# First, handle missing values\n",
        "df_model = df.copy()\n",
        "\n",
        "# Fill missing values with median\n",
        "df_model['income'] = df_model['income'].fillna(df_model['income'].median())\n",
        "df_model['spending_score'] = df_model['spending_score'].fillna(df_model['spending_score'].median())\n",
        "\n",
        "# Prepare features and target\n",
        "X = df_model.drop(['customer_id', 'high_value_customer'], axis=1)\n",
        "y = df_model['high_value_customer']\n",
        "\n",
        "# Convert categorical variables to dummy variables\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "print(f\"Feature columns: {list(X.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model trainer\n",
        "trainer = ModelTrainer()\n",
        "\n",
        "# Train multiple models for comparison\n",
        "print(\"Training multiple models...\")\n",
        "models = {}\n",
        "\n",
        "# Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "models['RandomForest'] = rf_model\n",
        "\n",
        "# Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "models['LogisticRegression'] = lr_model\n",
        "\n",
        "# Evaluate models\n",
        "print(\"\\nModel Evaluation Results:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    \n",
        "    # Detailed classification report\n",
        "    print(f\"\\nClassification Report for {name}:\")\n",
        "    print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance analysis (for Random Forest)\n",
        "if hasattr(rf_model, 'feature_importances_'):\n",
        "    # Create feature importance DataFrame\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'importance': rf_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.barplot(data=feature_importance.head(10), x='importance', y='feature')\n",
        "    plt.title('Top 10 Feature Importances (Random Forest)')\n",
        "    plt.xlabel('Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Top 10 Most Important Features:\")\n",
        "    print(feature_importance.head(10).round(4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Create confusion matrices for both models\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "for idx, (name, model) in enumerate(models.items()):\n",
        "    y_pred = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    \n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
        "    axes[idx].set_title(f'Confusion Matrix - {name}')\n",
        "    axes[idx].set_xlabel('Predicted')\n",
        "    axes[idx].set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Results and Conclusions\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. **Data Quality**: Our synthetic dataset had minimal quality issues, with only some missing values in income and spending score columns.\n",
        "\n",
        "2. **Feature Importance**: The most important features for predicting high-value customers are:\n",
        "   - Income levels\n",
        "   - Spending scores\n",
        "   - Number of purchases\n",
        "   - Years as customer\n",
        "\n",
        "3. **Model Performance**: Both Random Forest and Logistic Regression performed well, with Random Forest showing slightly better performance.\n",
        "\n",
        "### Recommendations\n",
        "\n",
        "1. **Data Collection**: Focus on collecting complete income and spending behavior data as these are strong predictors.\n",
        "\n",
        "2. **Model Deployment**: The Random Forest model can be deployed for identifying high-value customers.\n",
        "\n",
        "3. **Feature Engineering**: Consider creating interaction features between income, spending score, and purchase behavior.\n",
        "\n",
        "4. **Monitoring**: Set up regular model retraining as customer behavior patterns may change over time.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Hyperparameter Tuning**: Optimize model parameters using grid search or random search.\n",
        "2. **Cross-Validation**: Implement more robust cross-validation strategies.\n",
        "3. **Model Interpretability**: Use SHAP or LIME for better model interpretability.\n",
        "4. **A/B Testing**: Set up A/B tests to validate model performance in production.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best model for future use\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "\n",
        "# Save the Random Forest model\n",
        "model_path = '../models/high_value_customer_model.joblib'\n",
        "joblib.dump(rf_model, model_path)\n",
        "\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "print(\"Analysis complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
